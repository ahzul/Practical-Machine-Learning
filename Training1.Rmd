---
title: "PRACTICAL MACHINE LEARNING COURSE PROJECT"
author: "Zulkarnain Abu Hassan"
date: "January 14, 2016"
output: pdf_document
---

#1.SYNOPSIS
The goal of this project is to predict the manner in which they did the exercise based on the Training Dataset & Testing Dataset given

The report should describe:

•	“how you built your model”

•	“how you used cross validation”

•	“what you think the expected out of sample error is”

•	“why you made the choices you did”

Ultimately, the prediction model is to be run on the test data to predict the outcome of 20 different test cases.

In the aforementioned study, six participants participated in a dumbell lifting exercise five different ways. The five ways, as described in the study, were “exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.”

By processing data gathered from accelerometers on the belt, forearm, arm, and dumbell of the participants in a machine learning algorithm, the question is can the appropriate activity quality (class A-E) be predicted?

#2.Loading the appropriate packages

```{r}
library(caret)
library(ggplot2)
library(gridExtra)
library(grid)
library(randomForest)
library(rpart)
library(rpart.plot)
library(rattle) 
library(RColorBrewer) 
```

#3.Getting and loading the data

```{r}
set.seed(12345)

trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
```

#4.Partioning the training set into two

Partioning Training data set into two data sets, 60% for myTraining, 40% for myTesting:

```{r}
inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
```

#5.	Cleaning the data

a.	Remove NearZeroVariance variables

```{r}
nzv <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[,nzv$nzv==FALSE]

nzv<- nearZeroVar(myTesting,saveMetrics=TRUE)
myTesting <- myTesting[,nzv$nzv==FALSE]
```


b.	Remove the first column of the myTraining data set

```{r}
myTraining <- myTraining[c(-1)]
```


c.	Clean variables with more than 60% NA

```{r}
trainingV3 <- myTraining
for(i in 1:length(myTraining)) {
    if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .7) {
        for(j in 1:length(trainingV3)) {
            if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) == 1)  {
                trainingV3 <- trainingV3[ , -j]
            }   
        } 
    }
}

```

```{r}
# Set back to the original variable name
myTraining <- trainingV3
rm(trainingV3)

```


d.	Transform the myTesting and testing data sets

```{r}
clean1 <- colnames(myTraining)
# remove the classe column
clean2 <- colnames(myTraining[, -58])  
# allow only variables in myTesting that are also in myTraining
myTesting <- myTesting[clean1]         
# allow only variables in testing that are also in myTraining
testing <- testing[clean2]             

dim(myTesting)
dim(testing)
```


e.	Coerce the data into the same type

```{r}
for (i in 1:length(testing) ) {
    for(j in 1:length(myTraining)) {
        if( length( grep(names(myTraining[i]), names(testing)[j]) ) == 1)  {
            class(testing[j]) <- class(myTraining[i])
        }      
    }      
}

# To get the same class between testing and myTraining
testing <- rbind(myTraining[2, -58] , testing)
testing <- testing[-1,]

```


#6.	Prediction Analysis using

a.	Decision Trees

```{r}
set.seed(12345)
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(modFitA1)

```

```{r}
predictionsA1 <- predict(modFitA1, myTesting, type = "class")
cmtree <- confusionMatrix(predictionsA1, myTesting$classe)
cmtree

```

```{r}
plot(cmtree$table, col = cmtree$byClass, main = paste("Decision Tree Confusion Matrix: Accuracy =", round(cmtree$overall['Accuracy'], 4)))

```

b.	Random Forests

```{r}
set.seed(12345)
modFitB1 <- randomForest(classe ~ ., data=myTraining)
predictionB1 <- predict(modFitB1, myTesting, type = "class")
cmrf <- confusionMatrix(predictionB1, myTesting$classe)
cmrf

```

```{r}
plot(modFitB1)

```

```{r}
plot(cmrf$table, col = cmtree$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmrf$overall['Accuracy'], 4)))

```


#7. Predicting Results on the Test Data

Random Forests gave an Accuracy in the myTesting dataset of 99.89%, which was more accurate that what I got from the Decision Trees. The expected out-of-sample error is 100-99.89 = 0.11%.
